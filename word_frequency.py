#Â -*-Â coding:Â utf-8Â -*-
import os
import re
import json
import jsonlines
from nltk.stem import WordNetLemmatizer
wnl = WordNetLemmatizer()
# import thulac
# thuseg = thulac.thulac(seg_only=True, filt = False)
# import MeCab                                           #æ—¥è¯­åˆ†è¯
# mecab_tagger = MeCab.Tagger("-Owakati")


datafile = os.getcwd()+'/data_eng.jsonl'
file_result = os.getcwd() + "/corpus/"
# relevant_countries = ['us', 'uk', 'ca', 'au', 'nz', 'in', 'pk']
# relevant_countries = ['in']
# relevant_companies = ['h', 't', 'b']
# relevant_companies = ['b']
# badwords = ['çº¦ç‚®', 'çº¦ ç‚®', 'çˆ†ä¹³', 'æƒ…è¶£', 'è¿·å¥¸', 'å†…å°„', 'åç²¾', 'å ç²¾', 'è¡¥è‚¾', 'å·æƒ…', 'å¼ºå¥¸', 'æ‰å¥¸', 'è½®å¥¸', 'çŒ®å¦»', 'ç™½ç¿˜', 'å® å¹¸', 'å±çœ¼', 'AVç´ äºº', 'å•æ‹', 'è‚¥è‡€', 
#             'æ·«æ°´', 'å•ªå•ª', 'å¥³ä¼˜', 'å¥³ ä¼˜', 'å¢å¤§å¢ç²—', 'æ½®å¹', 'æ½® å¹', 'åƒå±Œ', 'åƒ å±Œ', 'è£¸ç…§', 'porn']
# stopwords_zh = ['ï¼Œ', 'çš„', 'ã€‚', 'ã€', 'ï¼š', 'å’Œ', 'åœ¨', 'ä¸­', 'æœ', 'å›½', 'äº†', 'æ˜¯', 'â€œ', 'â€', 'ä¸º', '#', ':', 'ï¼ˆ', 'ï¼‰', '35520398', 'ğŸ±@', '_', 'â€¦@', '.', 'o', 'å»',
#                 '(', ')', '-', ',', '|', 'å°±', 'ä»–ä»¬', '@', 'â€¦#', 'è¦', 'æœ‰', 'çœ‹', 'é‡Œ', 'åœ°', 'æ‰€', 'ä¸€', 'ğŸ±#', '/', 'ã€‘', 'å¯¹', 'ï¼', 'å¤ª', 'çœŸ', 'ç€', 'å¾ˆ', 'ã€', 'å…¥',
#                 'å€‘', 'å†™', 'å› ä¸º', 'ä¹°', 'æ¥', 'ä¹Ÿ', 'ã€‚\x00PLAYTHE']
# stopwords_ja = ['ã®', 'ãŒ', 'ã§', 'ã‚’', 'ã¯', '\x00@', 'ãŸ', 'ã«', 'ã¦', 'ã‚Œ', 'ã—', 'ï¼Œ', 'ã€‚', 'ã€', 'ï¼š', 'â€œ', 'â€', '#', ':', 'ï¼ˆ', 'ï¼‰', 'ğŸ±@', '_', 'â€¦@', '.', 'o', 
#                 '(', ')', '-', ',', '|', '@', 'â€¦#', 'ä¸€', 'ğŸ±#', '/', 'ã€‘', 'ã€']
# stopwords_ko = []
# stopwords = {'zh': stopwords_zh, 'ja': stopwords_ja, 'ko': stopwords_ko}
# ä¸éœ€è¦å†™å¤§å†™çš„åœç”¨è¯ï¼Œç¨‹åºé‡Œåˆ¤æ–­æ—¶å°†textéƒ½å˜å°å†™äº†ã€‚
stopwords = ['the', 'to', 'in', 'and', 'including', 'of', 'as', 'have', 'has', '&amp;', 'by', 'out', 'is', 'on', 'with', 'a', 'other', 'had', 'will', 'issues', 'issue',
            'several', 'be', 'auâ€¦@gauravcsawant:', 'already', 'citing', 'new', 'over', 'wechat.\x00noâ€¦@abhijitmajumder:', 'full', 'story:', 'but', 'are',
            'about', '-', 'looking', 'i', 'his', 'for', 'this', 'at','it', 'after', 'he', 'same', 'from', 'faces', 'added', "it,'", 'amid', "'we're", 'now', 'do', 'come',
            'comes', 'case', 'says', 'into', 'more', 'also', 'should', 'than', 'used', 'due', 'all', 'between', 'its', 'an', 'how', 'my', 'see', 'you', 'name', 'after',
            'not', "what's", 'what', '\x00\x00', 'that', 'another', 'one', 'why', 'here', 'can', 'may', 'going', 'want', 'where', 'would', 'or', 'they', 'it,', 
            'same,', 'was', 'use', 'let', 'get', 'we', 'shouldnâ€™t', "shouldn't", 'Itâ€™s', "It's", 'been', 'by', 'visited', 'visit', 'said', 'if', 'much', 'able', 'many',
            'say', 'big', 'which', 'set', 'some', 'make', 'made', 'makes', 'like', 'very']

# def delurl(text, url):
#     for i in range (len(url)):
#         text = text.replace(str(url[i]), '')
#     return text

# def dataprep(text, lang):
#     url = re.findall(r'http[a-zA-Z0-9\.\?\/\&\=\:\^\%\$\#\!]*', text)
#     text = delurl(text, url)
#     if lang == 'zh':
#         text = thuseg.cut(text, text=True)
#     if lang == 'ja':
#         text = mecab_tagger.parse(text)
#     text = text.replace("\n", "\0")
#     if text[0:2] == 'RT':
#         text_delRT = text[3:]
#     else:
#         text_delRT = text
#     return text_delRT

def words_frequency(inputfile, outputfile1, outputfile2, stopwords_):
    # è·å¾—è¯é¢‘
    print ("1ã€å¼€å§‹è®¡ç®—è¯é¢‘...")
    wordlist = inputfile.split()
    counted_words = []
    words_count = {}
    for i in range(len(wordlist)):                       # å…ˆæŠŠå¸¦'sçš„è¯çš„'så…¨å»æ‰
        if wordlist[i].endswith("'s"):
            wordlist[i] = wordlist[i][:-2]
        elif wordlist[i].endswith("'") or wordlist[i].endswith("â€™")  or wordlist[i].endswith(".")  or wordlist[i].endswith(","):
            wordlist[i] = wordlist[i][:-1]
    for i in range(len(wordlist)):
        lemmatized_word = wnl.lemmatize(wordlist[i])
        if lemmatized_word == 'ha':
            # print (wordlist[i])
            lemmatized_word = 'have'
        if lemmatized_word == 'wa':
            # print (wordlist[i])
            lemmatized_word = 'be'
        if lemmatized_word not in counted_words:
            counted_words.append(lemmatized_word)
            words_count[lemmatized_word] = 1
            for j in range(i+1, len(wordlist)):
                if wordlist[i] == wordlist[j]:
                    words_count[lemmatized_word] += 1
    # åˆå¹¶è¯ä¸å¯¹åº”é¢‘æ•°
    print ("2ã€åˆå¹¶è¯ä¸å¯¹åº”é¢‘æ•°...")
    words = []
    counts = []
    for items in words_count:
        words.append(items)
        counts.append(words_count[items])
    combined = list(zip(words, counts))
    # å†’æ³¡æ³•æ’å¤§å°ï¼Œè·å¾—æœªå»é™¤åœç”¨è¯çš„è¯é¢‘è¡¨
    print ("3ã€å†’æ³¡æ³•æ’å¤§å°...")
    for k in range(len(combined)-1):
        for i in range(len(combined)-1):
            if combined[i][1] < combined[i+1][1]:
                variable = combined[i]
                combined[i] = combined[i+1]
                combined[i+1] = variable
    with open (file_result + "{}.txt".format(outputfile1), "w", encoding='utf-8') as file:
        file.write(str(combined))
    # å»é™¤åœç”¨è¯åçš„è¯é¢‘è¡¨
    print ("4ã€å»é™¤åœç”¨è¯...")
    combined_no_stopwords = []
    for n in range(len(combined)):
        if combined[n][0].lower() in stopwords_:
            continue
        combined_no_stopwords.append(combined[n])
    with open (file_result + "{}.txt".format(outputfile2), "w", encoding='utf-8') as file:
        file.write(str(combined_no_stopwords))

print ("å¼€å§‹æ±‡æ€»è¯­æ®µï¼š")

full_text = ''
# print (pairs)
with open (datafile, 'r', encoding='utf-8') as f:
    count = 0
    try:
        for article in jsonlines.Reader(f):
            count += 1
            # text = dataprep(tweets['abstract'], 'en')
            assert article['year'] >= 2010 and article['year'] <= 2020
            text = article['abstract']
            full_text += text
            with open (file_result + "full_text_en" + ".txt", "a", encoding='utf-8') as file:
                file.write(text + ' ')
    except:
        print (count)


# with open (file_result + 'full_text_en.jsonl', "w", encoding='utf-8') as file:
#     # file.write(str(text_all))
#     file.write(json.dumps(full_text)+'\n')

print ("å¼€å§‹æ±‚è¯é¢‘ï¼š")

words_frequency(full_text, 'words_frequency_original', 'words_frequency_no_stopwords', stopwords)

print ("å®Œæˆï¼")
